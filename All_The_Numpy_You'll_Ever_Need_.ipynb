{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Let's start by importing numpy!"
      ],
      "metadata": {
        "id": "jj2kf4U2sSpm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 101,
      "metadata": {
        "id": "ypuEQ6qfsNGK"
      },
      "outputs": [],
      "source": [
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creating numpy arrays\n",
        "\n",
        "Alright, basics. `np.array()` creates numpy arrays from python lists."
      ],
      "metadata": {
        "id": "_fpJP7SEtfcd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = np.array([[1, 2], [2, 3], [3, 4]])\n",
        "print(f'{x = }')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aD1j3G2WsZTY",
        "outputId": "29c59ee3-41c1-4db0-f2d5-aa749fa52c8a"
      },
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x = array([[1, 2],\n",
            "       [2, 3],\n",
            "       [3, 4]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here are a few more common ways of creating numpy arrays:"
      ],
      "metadata": {
        "id": "4Mm-7hTMAymL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# similar to python range\n",
        "print(f'{np.arange(2, 10, 3) = }')\n",
        "print()\n",
        "\n",
        "print(f'{np.zeros((3, 2)) = }')\n",
        "print()\n",
        "\n",
        "print(f'{np.ones((3, 2)) = }')\n",
        "print()\n",
        "\n",
        "# tril => lower triangle,\n",
        "# useful for creating causal masks!\n",
        "print(f'{np.tril(np.ones((3,3))) = }')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y3mYctp4Aw_d",
        "outputId": "4001b7a6-6283-4f99-880c-12348bd3d74b"
      },
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "np.arange(2, 10, 3) = array([2, 5, 8])\n",
            "\n",
            "np.zeros((3, 2)) = array([[0., 0.],\n",
            "       [0., 0.],\n",
            "       [0., 0.]])\n",
            "\n",
            "np.ones((3, 2)) = array([[1., 1.],\n",
            "       [1., 1.],\n",
            "       [1., 1.]])\n",
            "\n",
            "np.tril(np.ones((3,3))) = array([[1., 0., 0.],\n",
            "       [1., 1., 0.],\n",
            "       [1., 1., 1.]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The `np.random` package can be used to create arrays with random numbers:"
      ],
      "metadata": {
        "id": "Hi6LWD8z_jY4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# random samples from a uniform\n",
        "# distribution over [0, 1), shape (3, 2)\n",
        "print(f'{np.random.rand(3, 2) = }')\n",
        "print()\n",
        "\n",
        "# random samples from the \"standard normal\"\n",
        "# distribution, shape (3, 2)\n",
        "print(f'{np.random.randn(3, 2) = }')\n",
        "print()\n",
        "\n",
        "# random sampled in the int range\n",
        "# [low, high), shape (3, 2)\n",
        "print(f'{np.random.randint(low=3, high=10, size=(3,2)) = }')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mnpezcY6_hrO",
        "outputId": "677a7b98-e95a-453a-ba20-3f1deeaa5918"
      },
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "np.random.rand(3, 2) = array([[0.80347779, 0.18355892],\n",
            "       [0.00679731, 0.75133837],\n",
            "       [0.82051179, 0.94778866]])\n",
            "\n",
            "np.random.randn(3, 2) = array([[-1.17867807,  0.29092374],\n",
            "       [ 1.1582176 ,  0.47130844],\n",
            "       [ 0.42378199,  0.01367287]])\n",
            "\n",
            "np.random.randint(low=3, high=10, size=(3,2)) = array([[7, 8],\n",
            "       [8, 7],\n",
            "       [9, 6]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## dtype and shape\n",
        "The shape and dtype of a numpy array are commonly used properties."
      ],
      "metadata": {
        "id": "7G_geMkYtKM0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'{x.shape = }')\n",
        "print(f'{x.dtype = }')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o0knzIEFtIkR",
        "outputId": "923ea374-549d-40d5-f4cf-251e833e2a6e"
      },
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x.shape = (3, 2)\n",
            "x.dtype = dtype('int64')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The shape and dtype of a numpy array can be changed to other compatible values, for instance:"
      ],
      "metadata": {
        "id": "KSqyBCO8yA3B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = np.array([[1, 2], [2, 3], [3, 4]])\n",
        "print(f'{x.shape = }')\n",
        "print(f'{x.dtype = }')\n",
        "\n",
        "# reshape\n",
        "x_23 = np.reshape(x, [2, 3])\n",
        "x_61 = x.reshape((-1, 1))  # numpy infers the value of -1, provided it's inferable.\n",
        "print(f'{x_23 = }, {x_23.shape = }')\n",
        "print(f'{x_61 = }, {x_61.shape = }')\n",
        "\n",
        "# change dtypes\n",
        "x_bool = np.array([True, False, False, True])\n",
        "x_int = x_bool.astype(np.int32)\n",
        "print(f'{x_int = }, {x_int.dtype = }')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M3aQqri8yAf9",
        "outputId": "4aca3717-32f8-4275-da7c-10aa2db307b4"
      },
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x.shape = (3, 2)\n",
            "x.dtype = dtype('int64')\n",
            "x_23 = array([[1, 2, 2],\n",
            "       [3, 3, 4]]), x_23.shape = (2, 3)\n",
            "x_61 = array([[1],\n",
            "       [2],\n",
            "       [2],\n",
            "       [3],\n",
            "       [3],\n",
            "       [4]]), x_61.shape = (6, 1)\n",
            "x_int = array([1, 0, 0, 1], dtype=int32), x_int.dtype = dtype('int32')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Indexing"
      ],
      "metadata": {
        "id": "nle35-5WtckE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's look at a few examples:"
      ],
      "metadata": {
        "id": "sBg7di1duvPy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = np.array([[1, 2], [2, 3], [3, 4]])\n",
        "print(f'{x = }')\n",
        "\n",
        "# access an element, python-style\n",
        "print(f'{x[0][1] = }')\n",
        "\n",
        "# access an element, numpy-style\n",
        "print(f'{x[0, 1] = }')\n",
        "\n",
        "# access a row\n",
        "print(f'{x[0] = }')\n",
        "\n",
        "# access a column\n",
        "print(f'{x[:, 0] = }')  # : means all the values from that axis"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NcSe0MldtdUZ",
        "outputId": "1abaa468-2f46-472c-f6b9-efab0e5d70ba"
      },
      "execution_count": 107,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x = array([[1, 2],\n",
            "       [2, 3],\n",
            "       [3, 4]])\n",
            "x[0][1] = np.int64(2)\n",
            "x[0, 1] = np.int64(2)\n",
            "x[0] = array([1, 2])\n",
            "x[:, 0] = array([1, 2, 3])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Even though python-style access and numpy-style access look identical, they can be deceptively different. Let's look at an example:"
      ],
      "metadata": {
        "id": "09HYEe5Lv25y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'{x[:, 0] = }')\n",
        "print(f'{x[:][0] = }')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2u__p7unvqvR",
        "outputId": "44cf12e8-3836-4f6b-8d99-cb7feb4501dc"
      },
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x[:, 0] = array([1, 2, 3])\n",
            "x[:][0] = array([1, 2])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "`x[:, 0]` returned the first column, but `x[:][0]` returned the first row. What happened here?\n",
        "\n",
        "`x[:][0]` creates a chain of two accesses: it first evaluates `x[:]`, which returns all of x, then evaluates res[0], which takes element 0 of the original array, hence returning the first row of the array. This behavior is consistent in python (`[[1, 2], [2, 3]][:][0] == [1, 2]`)\n",
        "\n",
        "`x[:, 0]` indexes both axes at once: all rows, column 0, and returns the first column."
      ],
      "metadata": {
        "id": "QKPKZiTmvqZo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's look at a few more examples of using `:` and `::` to access numpy arrays"
      ],
      "metadata": {
        "id": "e2W-ibl80m1r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = np.array([[1, 2], [2, 3], [3, 4], [4, 5], [5, 6], [6, 7]])\n",
        "print(f'{x = }')\n",
        "\n",
        "# access first 2 rows\n",
        "print(f'{x[:2] = }')\n",
        "\n",
        "# start at 1-th row, end at 4-th row\n",
        "print(f'{x[1:4] = }')\n",
        "\n",
        "# start at 1-th row, end at 4-th row, access every 2nd row\n",
        "print(f'{x[1:4:2] = }')\n",
        "\n",
        "# start at 0-th row, end at 4-th row, access every 2nd row\n",
        "print(f'{x[:4:2] = }')\n",
        "\n",
        "# start at 1-th row, end at the end, access every 2nd row\n",
        "print(f'{x[1::2] = }')\n",
        "\n",
        "# start at beginning, end at the end, access every 2nd row\n",
        "print(f'{x[::2] = }')\n",
        "\n",
        "# start at beginning, end at the end, access every -1-th row (in reverse)\n",
        "print(f'{x[::-1] = }')\n",
        "\n",
        "# start at beginning, end at the end, access every -1-th row and 0-th col\n",
        "print(f'{x[::-1, 0] = }')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tktmf14JzqQw",
        "outputId": "91e1222a-83c1-4c45-f77f-3eca61a32a2a"
      },
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x = array([[1, 2],\n",
            "       [2, 3],\n",
            "       [3, 4],\n",
            "       [4, 5],\n",
            "       [5, 6],\n",
            "       [6, 7]])\n",
            "x[:2] = array([[1, 2],\n",
            "       [2, 3]])\n",
            "x[1:4] = array([[2, 3],\n",
            "       [3, 4],\n",
            "       [4, 5]])\n",
            "x[1:4:2] = array([[2, 3],\n",
            "       [4, 5]])\n",
            "x[:4:2] = array([[1, 2],\n",
            "       [3, 4]])\n",
            "x[1::2] = array([[2, 3],\n",
            "       [4, 5],\n",
            "       [6, 7]])\n",
            "x[::2] = array([[1, 2],\n",
            "       [3, 4],\n",
            "       [5, 6]])\n",
            "x[::-1] = array([[6, 7],\n",
            "       [5, 6],\n",
            "       [4, 5],\n",
            "       [3, 4],\n",
            "       [2, 3],\n",
            "       [1, 2]])\n",
            "x[::-1, 0] = array([6, 5, 4, 3, 2, 1])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Unlike python, numpy arrays can also be indexed using integer lists:"
      ],
      "metadata": {
        "id": "WCNR1Z5an8Az"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = np.array([[1, 2], [2, 3], [3, 4], [4, 5], [5, 6], [6, 7]])\n",
        "print(f'{x = }')\n",
        "\n",
        "print(f'{x[[1, 3, 1]] = }')\n",
        "print(f'{x[np.array([1, 3, 5]), np.array([0, 1, 0])] = }')  # x[1, 0], x[3, 1], x[5, 0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p5KGSNq0n13F",
        "outputId": "98678d8f-4c9e-4848-b0fe-e4e997520b46"
      },
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x = array([[1, 2],\n",
            "       [2, 3],\n",
            "       [3, 4],\n",
            "       [4, 5],\n",
            "       [5, 6],\n",
            "       [6, 7]])\n",
            "x[[1, 3, 1]] = array([[2, 3],\n",
            "       [4, 5],\n",
            "       [2, 3]])\n",
            "x[np.array([1, 3, 5]), np.array([0, 1, 0])] = array([2, 5, 6])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This can be used to reorder arrays; the example below reorders array x in the decreasing order of array y. This is useful for instance when sampling from a language model, where the vocab ids have to be ordered by the probabilities generated by the model)\n",
        "\n"
      ],
      "metadata": {
        "id": "k37dSvhVrpBo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = np.random.permutation(5)\n",
        "y = np.random.randn(5)\n",
        "sorted_indices = np.argsort(x)\n",
        "reverse_sorted_indices = sorted_indices[::-1]\n",
        "\n",
        "print(f'{x = }')\n",
        "print(f'{y = }')\n",
        "print(f'{reverse_sorted_indices = }')\n",
        "print(f'{y[reverse_sorted_indices] = }')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RCOkii1drPp6",
        "outputId": "e26c9dea-dcc7-4a3d-8b79-9e97e18a364d"
      },
      "execution_count": 111,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x = array([4, 0, 3, 1, 2])\n",
            "y = array([ 0.0921816 ,  0.12551266, -0.5880462 , -0.45729852, -0.48287482])\n",
            "reverse_sorted_indices = array([0, 2, 4, 3, 1])\n",
            "y[reverse_sorted_indices] = array([ 0.0921816 , -0.5880462 , -0.48287482, -0.45729852,  0.12551266])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can also filter numpy arrays using bool lists or arrays for indexing:"
      ],
      "metadata": {
        "id": "fIXdK7CtoO9F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = np.array([[1, 2], [2, 3], [3, 4], [4, 5], [5, 6], [6, 7]])\n",
        "mask = np.array([True, False, False, True, True, False])\n",
        "print(f'{x = }')\n",
        "print(f'{mask = }')\n",
        "\n",
        "print(f'{x[mask] = }')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2CUyb6VgoOiC",
        "outputId": "e0db3fd0-1284-4b87-c20b-1d4a8b0e7339"
      },
      "execution_count": 112,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x = array([[1, 2],\n",
            "       [2, 3],\n",
            "       [3, 4],\n",
            "       [4, 5],\n",
            "       [5, 6],\n",
            "       [6, 7]])\n",
            "mask = array([ True, False, False,  True,  True, False])\n",
            "x[mask] = array([[1, 2],\n",
            "       [4, 5],\n",
            "       [5, 6]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This can be used to filter numpy arrays based on the values of (the same or other) numpy arrays, e.g. selecting model outputs belonging to a particular class of examples."
      ],
      "metadata": {
        "id": "SRMs11gZqrxa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = np.arange(-5, 5)\n",
        "y = np.random.randn(x.shape[0])\n",
        "print(f'{x = }')\n",
        "print(f'{y = }')\n",
        "print(f'{y[x > 2] = }')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jhib1fsIqe93",
        "outputId": "0aebe286-7a46-4adb-b583-5f393c9c2715"
      },
      "execution_count": 113,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x = array([-5, -4, -3, -2, -1,  0,  1,  2,  3,  4])\n",
            "y = array([-0.27360424,  2.29666033,  0.90999813, -0.61497772, -0.41640568,\n",
            "        0.47714379,  1.13851402, -1.70008134, -0.43164106,  0.90286183])\n",
            "y[x > 2] = array([-0.43164106,  0.90286183])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Numpy arrays are mutable, so all access patterns above can be used to set the values of the array at those indices (in contrast, jax arrays are immutable)."
      ],
      "metadata": {
        "id": "nvo6z-mMs74J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = np.array([[1, 2], [2, 3], [3, 4]])\n",
        "print(f'{x = }')\n",
        "\n",
        "print(\"# edit an element\")\n",
        "x[0][1] = -1\n",
        "print(f'{x = }')\n",
        "\n",
        "print(\"# edit a row\")\n",
        "x[0] = [-1, -2]\n",
        "print(f'{x = }')\n",
        "\n",
        "print(\"# edit a column\")\n",
        "x[:, 0] = [-1, -2, -3]\n",
        "print(f'{x = }')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vpX2TqQGs7Ke",
        "outputId": "9fb43ec8-72ce-465b-c3f5-19225314cb02"
      },
      "execution_count": 114,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x = array([[1, 2],\n",
            "       [2, 3],\n",
            "       [3, 4]])\n",
            "# edit an element\n",
            "x = array([[ 1, -1],\n",
            "       [ 2,  3],\n",
            "       [ 3,  4]])\n",
            "# edit a row\n",
            "x = array([[-1, -2],\n",
            "       [ 2,  3],\n",
            "       [ 3,  4]])\n",
            "# edit a column\n",
            "x = array([[-1, -2],\n",
            "       [-2,  3],\n",
            "       [-3,  4]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Broadcasting\n",
        "\n",
        "Let's look at a few simple operations."
      ],
      "metadata": {
        "id": "611RY55e2Abh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = np.array([[1, 2], [3, 4], [5, 6]])\n",
        "y = np.array([[2, 3], [4, 5], [6, 7]])\n",
        "\n",
        "print(f'{x = }')\n",
        "print(f'{y = }')\n",
        "print(f'{x + y = }')\n",
        "print(f'{x * y = }')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jV2IyqZk_c4e",
        "outputId": "64c02bc2-f31c-49b1-bfae-629cbe9babfe"
      },
      "execution_count": 115,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x = array([[1, 2],\n",
            "       [3, 4],\n",
            "       [5, 6]])\n",
            "y = array([[2, 3],\n",
            "       [4, 5],\n",
            "       [6, 7]])\n",
            "x + y = array([[ 3,  5],\n",
            "       [ 7,  9],\n",
            "       [11, 13]])\n",
            "x * y = array([[ 2,  6],\n",
            "       [12, 20],\n",
            "       [30, 42]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The above operations are element-wise (the operation is applied on each x[i, j] and y[i, j] pair) and easy to understand. What if shapes aren't the same?\n",
        "\n",
        "Numpy tries to broadcast the shape of the smaller array across the shape of the larger array to make them compatible. Here's the simplest example:"
      ],
      "metadata": {
        "id": "lzIyV2FRB-_R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = np.array([[1,2,3], [2,3,4], [4,5,6]])\n",
        "print(f'{x = }')\n",
        "print(f'{1 + x = }')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6sSMawW7wMYx",
        "outputId": "54f1cf71-524b-44cb-8060-2e4012132b96"
      },
      "execution_count": 116,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x = array([[1, 2, 3],\n",
            "       [2, 3, 4],\n",
            "       [4, 5, 6]])\n",
            "1 + x = array([[2, 3, 4],\n",
            "       [3, 4, 5],\n",
            "       [5, 6, 7]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here, the scalar 1 gets broadcasted to the shape of x, i.e. (3, 3) so it can be added to x. Let's look at a less simple example:"
      ],
      "metadata": {
        "id": "XVIkJjwgwjYX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = np.array([1, 2, 3])\n",
        "y = np.array([[1, 2, 3], [2, 3, 4], [4, 5, 6]])\n",
        "print(f'{x = }, {x.shape = }')\n",
        "print(f'{y = }, {y.shape = }')\n",
        "print(f'{x + y = }')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DauQWlKfB-dI",
        "outputId": "3dcebf94-ff1c-4077-9e1e-c0db0387036f"
      },
      "execution_count": 117,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x = array([1, 2, 3]), x.shape = (3,)\n",
            "y = array([[1, 2, 3],\n",
            "       [2, 3, 4],\n",
            "       [4, 5, 6]]), y.shape = (3, 3)\n",
            "x + y = array([[2, 4, 6],\n",
            "       [3, 5, 7],\n",
            "       [5, 7, 9]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "X was replicated across axis = 0, so the above operation was equivalent to `[[1, 2, 3], [2, 3, 4], [4, 5, 6]] + [[1, 2, 3], [1, 2, 3], [1, 2, 3]]`. What if we wanted to replcate X across axis = 0, and perform the equivalent of `[[1, 2, 3], [2, 3, 4], [4, 5, 6]] + [[1, 1, 1], [2, 2, 2], [3, 3, 3]]`?\n",
        "\n",
        "We can do this without making needless copies of the smaller array, by adding new dimensions to the array. Here are 3 equivalent ways to do this:"
      ],
      "metadata": {
        "id": "e3GVXj8w1YTH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = np.array([1, 2, 3])\n",
        "print(f'{x = }, {x.shape = }')\n",
        "\n",
        "print()\n",
        "\n",
        "print(f'{x[None, :] = }, {x[None, :].shape = }')\n",
        "print(f'{x[np.newaxis, :] = }, {x[np.newaxis, :].shape = }')\n",
        "print(f'{np.expand_dims(x, axis=0) = }, {np.expand_dims(x, axis=0).shape = }')\n",
        "\n",
        "print()\n",
        "\n",
        "print(f'{x[:, None] = }, {x[:, None].shape = }')\n",
        "print(f'{x[:, np.newaxis] = }, {x[:, np.newaxis].shape = }')\n",
        "print(f'{np.expand_dims(x, axis=1) = }, {np.expand_dims(x, axis=1).shape = }')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ohbdQMDLzV25",
        "outputId": "dbb71aea-5b05-4211-e02e-f4ea28541222"
      },
      "execution_count": 118,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x = array([1, 2, 3]), x.shape = (3,)\n",
            "\n",
            "x[None, :] = array([[1, 2, 3]]), x[None, :].shape = (1, 3)\n",
            "x[np.newaxis, :] = array([[1, 2, 3]]), x[np.newaxis, :].shape = (1, 3)\n",
            "np.expand_dims(x, axis=0) = array([[1, 2, 3]]), np.expand_dims(x, axis=0).shape = (1, 3)\n",
            "\n",
            "x[:, None] = array([[1],\n",
            "       [2],\n",
            "       [3]]), x[:, None].shape = (3, 1)\n",
            "x[:, np.newaxis] = array([[1],\n",
            "       [2],\n",
            "       [3]]), x[:, np.newaxis].shape = (3, 1)\n",
            "np.expand_dims(x, axis=1) = array([[1],\n",
            "       [2],\n",
            "       [3]]), np.expand_dims(x, axis=1).shape = (3, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's use this to \"guide\" how x is broadcasted across y:"
      ],
      "metadata": {
        "id": "5SSVja5lz_H6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = np.array([1, 2, 3])\n",
        "y = np.array([[1, 2, 3], [2, 3, 4], [4, 5, 6]])\n",
        "print(f'{x = }, {x.shape = }')\n",
        "print(f'{y = }, {y.shape = }')\n",
        "print(f'{x[None, :] + y = }')\n",
        "print(f'{x[:, None] + y = }')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XIGyDYmQ1XAW",
        "outputId": "59555315-989d-46a1-b08a-64d4a7caf954"
      },
      "execution_count": 119,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x = array([1, 2, 3]), x.shape = (3,)\n",
            "y = array([[1, 2, 3],\n",
            "       [2, 3, 4],\n",
            "       [4, 5, 6]]), y.shape = (3, 3)\n",
            "x[None, :] + y = array([[2, 4, 6],\n",
            "       [3, 5, 7],\n",
            "       [5, 7, 9]])\n",
            "x[:, None] + y = array([[2, 3, 4],\n",
            "       [4, 5, 6],\n",
            "       [7, 8, 9]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Broadcasting is pretty commonly used in numpy code, e.g. broadcasting the bias across a batch of features in an linear layer, but can be tricky. Always test your broadcasting code on small examples to make sure it's working correctly!"
      ],
      "metadata": {
        "id": "CG400qCe0KRd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Let's use numpy!\n",
        "Let's use numpy to implement some components commonly used in neural networks! This will introduce us to a few common numpy functions and help us get familiar with the indexing and broadcasting concepts we just read about.\n",
        "\n",
        "Please feel free to look up the expressions on the internet and try implementing these yourself and use the tests to verify your approach before looking at the implementations provided here!"
      ],
      "metadata": {
        "id": "1kQX-pv-NaOk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ReLU\n",
        "\n",
        "ReLU (rectified linear unit) is a basic activation function used in neural networks and is defined as relu(x) = x if x > 0 else 0. Let's implement it in numpy!"
      ],
      "metadata": {
        "id": "pdZGxiU_L0Ec"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def relu(x):\n",
        "  return np.where(x > 0, x, 0)"
      ],
      "metadata": {
        "id": "xxCKimteL7A6"
      },
      "execution_count": 120,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's test it:"
      ],
      "metadata": {
        "id": "4y0lbHx6NC1e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_tests = 5\n",
        "for _ in range(num_tests):\n",
        "  x = np.random.permutation(np.arange(-5, 5))\n",
        "  print(f'{x = }')\n",
        "  print(f'{relu(x) = }')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X0cWXGezNDnG",
        "outputId": "852317b7-bc8b-4e73-e4d9-2dfc94ab5f8c"
      },
      "execution_count": 121,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x = array([ 3,  2, -1, -3, -2, -4,  4,  0,  1, -5])\n",
            "relu(x) = array([3, 2, 0, 0, 0, 0, 4, 0, 1, 0])\n",
            "x = array([-1,  2,  4, -5,  3, -2,  1, -3,  0, -4])\n",
            "relu(x) = array([0, 2, 4, 0, 3, 0, 1, 0, 0, 0])\n",
            "x = array([-4,  0,  3, -2,  4, -1,  2, -5,  1, -3])\n",
            "relu(x) = array([0, 0, 3, 0, 4, 0, 2, 0, 1, 0])\n",
            "x = array([ 3, -2,  0, -4, -5, -3, -1,  2,  4,  1])\n",
            "relu(x) = array([3, 0, 0, 0, 0, 0, 0, 2, 4, 1])\n",
            "x = array([ 3,  2,  4, -3, -2,  1, -1, -4,  0, -5])\n",
            "relu(x) = array([3, 2, 4, 0, 0, 1, 0, 0, 0, 0])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can also implement relu(x) as `np.maximum(x, 0)`, which returns the element-wise maximum of the two arrays x and 0 (broadcasted). This is different from `np.max(a)` returns the max of array a (optionally, along an axis). Try modifying the implementation and run the tests!\n",
        "\n",
        "BTW, `relu(x) = x[x > 0]` is incorrect. Can you explain why? Modify the function and run the tests to see what happens."
      ],
      "metadata": {
        "id": "AVbcwT5gMZkd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Sigmoid\n",
        "Sigmoid is used to convert the model output (i.e. logit) to a probability, commonly used for binary classfication problems. Let's implement it!"
      ],
      "metadata": {
        "id": "O3T482q6Ly6X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sigmoid(x):\n",
        "  return 1 / (1 + np.exp(-x))"
      ],
      "metadata": {
        "id": "vAwI_XQBOZpu"
      },
      "execution_count": 122,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "It's harder to inspect correctness, but let's run a few tests anyway!"
      ],
      "metadata": {
        "id": "60E6GMmGOnpD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_tests = 5\n",
        "for _ in range(num_tests):\n",
        "  x = np.random.randn(5)\n",
        "  print(f'{x = }')\n",
        "  print(f'{sigmoid(x) = }')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c2NctJQNOfBv",
        "outputId": "b8a5cbaa-13bf-4c67-a631-f459d93d2320"
      },
      "execution_count": 123,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x = array([1.37604244, 1.11145492, 0.60292134, 0.82949437, 0.00775282])\n",
            "sigmoid(x) = array([0.79835465, 0.75240025, 0.64632438, 0.69624801, 0.50193819])\n",
            "x = array([ 0.18027862,  0.03980114,  0.18315271,  0.62267815, -1.96071236])\n",
            "sigmoid(x) = array([0.54494799, 0.50994897, 0.54566061, 0.65082741, 0.12338997])\n",
            "x = array([ 0.24902036,  0.25138283, -0.59668984, -0.12613275,  0.44219321])\n",
            "sigmoid(x) = array([0.56193536, 0.56251683, 0.35510137, 0.46850855, 0.6087815 ])\n",
            "x = array([ 1.76297071,  0.54180733,  0.13112122, -0.70372507, -0.94553125])\n",
            "sigmoid(x) = array([0.85358133, 0.63223275, 0.53273342, 0.33098685, 0.27978441])\n",
            "x = array([-1.23177664,  0.2778366 ,  0.9518457 , -1.05632614, -1.30979448])\n",
            "sigmoid(x) = array([0.22587062, 0.56901576, 0.72148621, 0.25801216, 0.21252124])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "One visual check for correctness is that the sigmoid is always positive (can you explain why?). We can also check for correctness using a few known values, e.g. x = 0, x = inf, and x = -inf. Can you explain the following outputs?"
      ],
      "metadata": {
        "id": "vKmyOZPJOv_o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "  print(f'{sigmoid(0) = }')  # 0.5\n",
        "  print(f'{sigmoid(-np.inf) = }')  # 0.0\n",
        "  print(f'{sigmoid(np.inf) = }')  # 1.0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8IrawslmOs29",
        "outputId": "a0e07c8d-8060-426e-961c-c8958ef2f9ad"
      },
      "execution_count": 124,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sigmoid(0) = np.float64(0.5)\n",
            "sigmoid(-np.inf) = np.float64(0.0)\n",
            "sigmoid(np.inf) = np.float64(1.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Softmax"
      ],
      "metadata": {
        "id": "PaQyaBW5Lt9C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The softmax function is used to convert model logits to probabilities for multi-label classification problems. For instance, when sampling from a language model, the model outputs a logit for each item in the vocabulary; these are converted to probabilities using softmax and sampled from (we'll learn more about sampling techniques later!)\n",
        "\n",
        "Let's implement it!"
      ],
      "metadata": {
        "id": "04g5XpnmlLIl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def softmax(x, axis=None):\n",
        "  exp = np.exp(x)\n",
        "  sumexp = np.sum(exp, axis=axis, keepdims=True)\n",
        "  return exp / sumexp"
      ],
      "metadata": {
        "id": "L4MxVEL6Pd_d"
      },
      "execution_count": 125,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's test this before diving into the details:"
      ],
      "metadata": {
        "id": "VipxtFSdSPfN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = np.array([[1, 1], [3, 0]])\n",
        "print(f'{x = }')\n",
        "print(f'{softmax(x) = }')\n",
        "\n",
        "x = np.array([[1, 2], [3, 0]])\n",
        "print(f'{x = }')\n",
        "print(f'{softmax(x, axis=0) = }')\n",
        "\n",
        "x = np.array([[1, 2], [3, 0]])\n",
        "print(f'{x = }')\n",
        "print(f'{softmax(x, axis=1) = }')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z7Li--jcSMM2",
        "outputId": "390da4a9-743f-416f-c93b-7192fbc3fdaa"
      },
      "execution_count": 126,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x = array([[1, 1],\n",
            "       [3, 0]])\n",
            "softmax(x) = array([[0.1024912, 0.1024912],\n",
            "       [0.7573132, 0.0377044]])\n",
            "x = array([[1, 2],\n",
            "       [3, 0]])\n",
            "softmax(x, axis=0) = array([[0.11920292, 0.88079708],\n",
            "       [0.88079708, 0.11920292]])\n",
            "x = array([[1, 2],\n",
            "       [3, 0]])\n",
            "softmax(x, axis=1) = array([[0.26894142, 0.73105858],\n",
            "       [0.95257413, 0.04742587]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are a few things to talk about here, let's break them down step by step.\n",
        "\n",
        "Let's first talk about the axis.\n",
        "\n",
        "Many numpy operations allow you to apply them over one or more (or all) axes of the array. axis=None applies the operation over the entire array. But, commonly you'd apply softmax over a batch of activations (e.g. of shape (B, H), where B is the batch size and H is the hidden dimension) and here you need to apply softmax over each row of features separately rather than the entire array.\n",
        "\n",
        "Compare the examples above:\n",
        "+ #1 applies softmax to the entire array; all the values of the softmax add up to 1.\n",
        "+ #2 applies softmax to each column of the array; all the values of the softmax in each col add up to 1.\n",
        "+ #3 applies softmax to each row separately; all the values of the softmax in each row add up to 1. This is what we want in the (B, H) case.\n",
        "\n",
        "So, axis=k denotes which dimension of the array to apply the operation over. If the shape of the array is (x0, x1, ..., xn), then axis=k applies the operation over the xk dim of the array. In the 2-D case discussed above, axis=0 applies it to the \"batch\" dim (cols), and axis=1 applies it the \"feature\" dim (rows).\n",
        "\n",
        "Let's now talk about `keepdims`.\n",
        "\n",
        "When we apply \"reduce\" operations like sum, max, etc. `num_output_dims = num_input_dims - num_axes_applied_by_op`. Setting `keepdims=True` expands the output dims to force `num_output_dims = num_input_dims`. For instance, for x of shape (B, H), the output has shape:\n",
        "  + for axis=None (sum over all axes), scalar for keepdims=False, (1, 1) for keepdims=True.\n",
        "  + for axis=0, (H,) for keepdims=False, (1, H) for keepdims=True.\n",
        "  + for axis=1, (B,) for keepdims=False, (B, 1) for keepdims=True.\n",
        "\n",
        "Let's see this in action:"
      ],
      "metadata": {
        "id": "RuGraHTeP2z1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = np.array([[1, 1], [3, 0]])\n",
        "\n",
        "print(f'{x = }, {x.shape = }')\n",
        "for axis in (None, 0, 1):\n",
        "  print(f'{axis = }')\n",
        "  sum_nokeepdim = np.sum(x, axis=axis, keepdims=False)\n",
        "  sum_keepdim = np.sum(x, axis=axis, keepdims=True)\n",
        "  print(f'{np.sum(x, axis=axis, keepdims=False) = }, {sum_nokeepdim.shape = }')\n",
        "  print(f'{np.sum(x, axis=axis, keepdims=True) = }, {sum_keepdim.shape = }')\n",
        "\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KmWr8X0MTdmg",
        "outputId": "6fd008b2-52ba-4e53-9718-bc2b7002d8f5"
      },
      "execution_count": 127,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x = array([[1, 1],\n",
            "       [3, 0]]), x.shape = (2, 2)\n",
            "axis = None\n",
            "np.sum(x, axis=axis, keepdims=False) = np.int64(5), sum_nokeepdim.shape = ()\n",
            "np.sum(x, axis=axis, keepdims=True) = array([[5]]), sum_keepdim.shape = (1, 1)\n",
            "axis = 0\n",
            "np.sum(x, axis=axis, keepdims=False) = array([4, 1]), sum_nokeepdim.shape = (2,)\n",
            "np.sum(x, axis=axis, keepdims=True) = array([[4, 1]]), sum_keepdim.shape = (1, 2)\n",
            "axis = 1\n",
            "np.sum(x, axis=axis, keepdims=False) = array([2, 3]), sum_nokeepdim.shape = (2,)\n",
            "np.sum(x, axis=axis, keepdims=True) = array([[2],\n",
            "       [3]]), sum_keepdim.shape = (2, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Why is it important to set `keepdims=True`? To get the softmax, we divide the exponents of shape (B, H) by the sum of exponents. For axis=None, sumexp is a scalar and trivially broadcasted over shape (B, H). But for axis=0 and axis=1, the shape of sumexp (H,) or (B,) respectively, will be broadcasted over over shape (B, H) and this would fail for axis=1, or worse, if B == H, the sum will be broadcasted over the wrong dim and fail silently. Hence, we guide the broadcasting by setting keepdims=True. BTW, this is equivalent to `sum = np.sum(x, axis=1, keepdims=False); sum = sum[:, None]`\n",
        "\n",
        "\n",
        "Let's check it out with our example (B = H = 2). Can you explain why the answer is wrong with keepdims=False?"
      ],
      "metadata": {
        "id": "3YT6-xHyYj9d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = np.array([[1, 1], [3, 0]])\n",
        "\n",
        "print(f'{x = }, {x.shape = }')\n",
        "for axis in (1,):\n",
        "  print(f'{axis = }')\n",
        "  sum_nokeepdim = np.sum(x, axis=axis, keepdims=False)\n",
        "  sum_keepdim = np.sum(x, axis=axis, keepdims=True)\n",
        "  print(f'{np.sum(x, axis=axis, keepdims=False) = }, {sum_nokeepdim.shape = }')\n",
        "  print(f'{np.sum(x, axis=axis, keepdims=True) = }, {sum_keepdim.shape = }')\n",
        "  print(f'Wrong: {x / sum_nokeepdim = }')\n",
        "  print(f'Correct: {x / sum_keepdim = }')\n",
        "  print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8umThb9BYjrZ",
        "outputId": "c5683c91-dfdd-4179-9f43-e9575c2c8651"
      },
      "execution_count": 128,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x = array([[1, 1],\n",
            "       [3, 0]]), x.shape = (2, 2)\n",
            "axis = 1\n",
            "np.sum(x, axis=axis, keepdims=False) = array([2, 3]), sum_nokeepdim.shape = (2,)\n",
            "np.sum(x, axis=axis, keepdims=True) = array([[2],\n",
            "       [3]]), sum_keepdim.shape = (2, 1)\n",
            "Wrong: x / sum_nokeepdim = array([[0.5       , 0.33333333],\n",
            "       [1.5       , 0.        ]])\n",
            "Correct: x / sum_keepdim = array([[0.5, 0.5],\n",
            "       [1. , 0. ]])\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "A general rule of thumb is that numpy can broadcast over the batch dimension, but not others. So (H,) -> (B, H) is correctly done without guidance, but (B,) -> (B, H) has to be guided using res[:, None] (or np.newaxis or np.expand_dims).\n",
        "\n",
        "Great, hopefully your understanding of axes and broadcasting was enriched by this example! Let's apply it to improve the numerical stability of our softmax impl."
      ],
      "metadata": {
        "id": "G5zxPn3ScScH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Stable Softmax\n",
        "In our prev softmax impl, the sumexp term can become really large. To counter this, we subtract x by the the max of x (along the provided axis) before computing the exponents and summing them. This is equivalent to multiplying both the numerator and denominator of the softmax by exp(-mx) and so has no effect on the output of the softmax. But because the input to exp is now in the range (-inf, 0), the exp will be in range (0, 1) which makes the sumexp manageable.\n",
        "\n",
        "Let's implement this!"
      ],
      "metadata": {
        "id": "h43cL7PZm-U0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def stable_softmax(x, axis=None):\n",
        "  mx = np.max(x, axis=axis, keepdims=True)\n",
        "  exps = np.exp(x - mx)\n",
        "  sumexps = np.sum(exps, axis=axis, keepdims=True)\n",
        "  return exps / sumexps"
      ],
      "metadata": {
        "id": "MdGsU3JZdOjd"
      },
      "execution_count": 129,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_tests = 5\n",
        "B = 5\n",
        "H = 10\n",
        "for i in range(num_tests):\n",
        "  x = np.random.randn(B, H)\n",
        "  s1 = softmax(x, axis=-1)\n",
        "  s2 = stable_softmax(x, axis=-1)\n",
        "  np.testing.assert_allclose(s1, s2)\n",
        "print(\"The two impls matched!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RhykAtrRdZvH",
        "outputId": "68bd5b87-5c41-4e0f-e036-65a739ed05fd"
      },
      "execution_count": 130,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The two impls matched!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note that as with indexing, `axis=-k` is equivalent to `axis=num_axes-k`. `axis=-1` is often used when you want to apply an operation to the feature dimension but there might be multiple \"batch\" dimensions, e.g. with language models, you often have a batch of token seqeunces of shape (B, T, H). Let's see this in action in LayerNorm!"
      ],
      "metadata": {
        "id": "087kDKL0dpYj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### LayerNorm"
      ],
      "metadata": {
        "id": "8HIOJg0gLwLq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Layernorm is a common component of transformer architectures (recently RMSNorm has been more common because it has fewer learnable params, hence simpler).\n",
        "\n",
        "For layernorm, we compute the mean and variance along the feature axis (i.e. for each example in the batch) and normalize the features based on these. We also have 2 learnable params to scale and shift the normalized features; for now we'll assume that these are constants, and we'll later see how to apply backprop and update (i.e. learn) these params. Let's go!"
      ],
      "metadata": {
        "id": "IBUs3blHlGqp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def layernorm(x):\n",
        "  B, T, H = x.shape\n",
        "  mean_BT = np.mean(x, axis=-1, keepdims=True)\n",
        "  var_BT = np.var(x, axis=-1, keepdims=True)\n",
        "  eps = 1e-8\n",
        "  norm_x_BTH = (x - mean_BT) / np.sqrt(var_BT + eps)\n",
        "  # learnable params\n",
        "  scale_H = np.ones((H,))\n",
        "  shift_H = np.zeros((H,))\n",
        "  return norm_x_BTH * scale_H + shift_H"
      ],
      "metadata": {
        "id": "swZtpu8mLtO5"
      },
      "execution_count": 131,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's test this! Again, it's hard to visually inspect for correctness, but let's create an input array of large numbers so that we can see the effect of normalization (i.e. output values are close to 0). In our impl, the scale (= 1) and shift (= 0) have no effect because we're multiplying by 1 and shifting by 0."
      ],
      "metadata": {
        "id": "BAfhrz90rTRW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "B = 2\n",
        "T = 3\n",
        "H = 4\n",
        "\n",
        "x = np.random.randn(B, T, H)\n",
        "# scale and shift by large numbers so that we can see the effect of layernorm\n",
        "x = x * 100 + 200\n",
        "print(f'{x = }')\n",
        "print(f'{layernorm(x) = }')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2duR4bSBmQ4t",
        "outputId": "e13f8dbd-e6b6-492e-9df0-b8f058c5d78e"
      },
      "execution_count": 132,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x = array([[[ 12.11184928, 185.13998198, 149.66436125, 226.30952782],\n",
            "        [340.55772866, 163.6211746 , 145.26466405, 369.67840459],\n",
            "        [362.28143825, 278.00379825, 217.88968202, 217.71828902]],\n",
            "\n",
            "       [[199.36509277, 383.66238749,  68.57137326, 179.72458591],\n",
            "        [507.60666486, 328.1334632 , 203.82862929, 360.45701888],\n",
            "        [240.43294338, 371.42847709,  10.8403051 , 204.01936092]]])\n",
            "layernorm(x) = array([[[-1.63065873,  0.51996238,  0.07902473,  1.03167162],\n",
            "        [ 0.8486662 , -0.90191566, -1.08353203,  1.13678149],\n",
            "        [ 1.57581316,  0.15250948, -0.86271405, -0.86560858]],\n",
            "\n",
            "       [[-0.07484831,  1.5545776 , -1.23123356, -0.24849573],\n",
            "        [ 1.45711692, -0.20222999, -1.35150932,  0.09662239],\n",
            "        [ 0.26149626,  1.27637422, -1.51725528, -0.0206152 ]]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As an exercise, try implementing BatchNorm (hint: look at the axis!) and RMSNorm (hint: use `np.linalg.norm`!)\n",
        "\n",
        "Remember our rule of thumb regarding broadcasting: numpy knows how to broadcast over the batch dim. Hence we use keepdims=True to guide the broadcasting of mean and var because these are broadcasted over the feature dim (axis=2 or axis=-1), but we don't need to guide the broadcasting of the scale and shift params which are broadcasted over the batch dims. Hence `norm_x_BTH * scale_H[None, None, :] + shift_H[None, None, :]` is redundant.\n"
      ],
      "metadata": {
        "id": "_AAWzqoyqlzH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Matrix Multiplication"
      ],
      "metadata": {
        "id": "knZTfJO-1UEk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you're rusty on matrix multiplication, do a quick google search and familiarize yourself with the concept.\n",
        "\n",
        "Let's look a few common matrix multiplications we'll encounter in practice.\n",
        "\n",
        "One scenario is projecting a batch of activations from one dimension (e.g. model dimension) to another (e.g. hidden dimensions). This involves multiplying an activations array of shape (B, F) and a (learnable) weight array of shape (F, H) to get an activations array of shape (B, H), where B -> batch dim, F -> feature dim and H -> hidden dim. Let's implement this in 4 ways!"
      ],
      "metadata": {
        "id": "ZH6T7lZ911vE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "B, F, H = 2, 3, 4\n",
        "x = np.random.randn(B, F)\n",
        "y = np.random.randn(F, H)\n",
        "print(f'{x = }')\n",
        "print(f'{y = }')\n",
        "\n",
        "print()\n",
        "\n",
        "print(f'{np.dot(x, y) = }')\n",
        "print(f'{np.matmul(x, y) = }')\n",
        "print(f'{x@y = }')\n",
        "print(f'{np.einsum(\"bf,fh->bh\", x, y) = }')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3P_AaE3O1WWP",
        "outputId": "e4ab7b98-89d7-48ee-b670-6aabfed67733"
      },
      "execution_count": 133,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x = array([[-0.92906697, -0.01587976,  0.51553643],\n",
            "       [ 0.2075524 ,  1.39131833,  0.26085643]])\n",
            "y = array([[-0.83932852, -0.75563547,  0.55694521, -1.93553376],\n",
            "       [ 0.02379483, -0.25176599,  1.20980513,  0.60516133],\n",
            "       [-0.73326398,  0.74425326, -0.7110913 , -0.49482402]])\n",
            "\n",
            "np.dot(x, y) = array([[ 0.40139026,  1.08972362, -0.90324429,  1.53353086],\n",
            "       [-0.33237509, -0.31297734,  1.61332664,  0.31116934]])\n",
            "np.matmul(x, y) = array([[ 0.40139026,  1.08972362, -0.90324429,  1.53353086],\n",
            "       [-0.33237509, -0.31297734,  1.61332664,  0.31116934]])\n",
            "x@y = array([[ 0.40139026,  1.08972362, -0.90324429,  1.53353086],\n",
            "       [-0.33237509, -0.31297734,  1.61332664,  0.31116934]])\n",
            "np.einsum(\"bf,fh->bh\", x, y) = array([[ 0.40139026,  1.08972362, -0.90324429,  1.53353086],\n",
            "       [-0.33237509, -0.31297734,  1.61332664,  0.31116934]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note that `np.matmul` and `@` are exactly equivalent, but different from `np.dot` and `np.einsum`."
      ],
      "metadata": {
        "id": "VQPxPSjJqu5D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Another scenario is projecting a batch of activations from the feature dimension to a scalar for a binary classifier. This involves multiplying an activations array of shape (B, F) and a (learnable) weight array of shape (F,) to get an activations array of shape (B,). This is generally passed through sigmoid to get a batch of probabilities. Let's implement this in 4 ways!"
      ],
      "metadata": {
        "id": "MONwhu0JLMZi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "B, F = 2, 3\n",
        "x = np.random.randn(B, F)\n",
        "y = np.random.randn(F)\n",
        "print(f'{x = }')\n",
        "print(f'{y = }')\n",
        "\n",
        "print()\n",
        "\n",
        "print(f'{np.dot(x, y) = }')\n",
        "print(f'{np.matmul(x, y) = }')\n",
        "print(f'{x@y = }')\n",
        "print(f'{np.einsum(\"bf,f->b\", x, y) = }')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "32PtvmUZTw4V",
        "outputId": "e82a6267-b649-4147-c75f-230808c7c013"
      },
      "execution_count": 134,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x = array([[ 1.65066275, -0.06141763,  0.46269886],\n",
            "       [ 0.0817957 , -0.61923632, -1.5447    ]])\n",
            "y = array([-2.28078   ,  0.37872225, -1.00676131])\n",
            "\n",
            "np.dot(x, y) = array([-4.25388613,  1.13406764])\n",
            "np.matmul(x, y) = array([-4.25388613,  1.13406764])\n",
            "x@y = array([-4.25388613,  1.13406764])\n",
            "np.einsum(\"bf,f->b\", x, y) = array([-4.25388613,  1.13406764])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's look an example with more dimensions. In language models, we multiply a batch of token activations of shape (B, T, F) with a (learned) multi-headed query projection matrix of shape (N, F, H) to get a batch of multi-headed queries of shape (B, T, N, H) where B -> batch dim, T -> sequence length, F -> feature dim, N -> number of query heads and H -> attention dim.\n",
        "\n",
        "Let's see what happens when we use `np.matmul` (and equivalently, `@`):"
      ],
      "metadata": {
        "id": "GWKpS_BgUBXa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "B, T, F, N, H = 2, 3, 6, 2, 3\n",
        "x = np.random.randn(B, T, F)\n",
        "y = np.random.randn(N, F, H)\n",
        "print(f'{x = }')\n",
        "print(f'{y = }')\n",
        "\n",
        "print(f'{np.matmul(x, y) = }')\n",
        "print(f'{x@y = }')\n",
        "print(f'{np.matmul(x, y).shape = }')\n",
        "print(f'{(x@y).shape = }')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MUreeWjtUBEM",
        "outputId": "05996ad0-a3d7-4262-df3d-7f5a6ee62303"
      },
      "execution_count": 135,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x = array([[[ 0.41390555,  0.13638431,  0.62620411,  2.56351415,\n",
            "         -0.48740992, -0.53257156],\n",
            "        [-0.57644863,  0.45416635,  0.13687466,  0.29902331,\n",
            "         -0.31945657,  0.03636417],\n",
            "        [ 1.69296709, -0.14465308, -0.33442766,  0.72543638,\n",
            "         -0.58268987, -0.63492938]],\n",
            "\n",
            "       [[-0.1212688 ,  2.27309343, -0.85056702,  0.43697212,\n",
            "          2.0249621 , -0.16939108],\n",
            "        [-0.11536122,  1.35445297, -0.07966449, -1.5367472 ,\n",
            "          0.4832491 , -0.71941564],\n",
            "        [ 0.31344658,  2.71318022,  1.17496961, -0.40654459,\n",
            "          0.1027015 , -0.88181405]]])\n",
            "y = array([[[-0.22258907, -1.10398432, -0.83395839],\n",
            "        [-1.26182501,  0.23372378, -0.22390359],\n",
            "        [ 0.47326375, -0.58355301, -0.61470167],\n",
            "        [ 0.58316526, -1.20394121,  0.18945354],\n",
            "        [-0.73882863, -1.24806251,  0.19391437],\n",
            "        [-0.5234439 , -0.08236514, -0.46774193]],\n",
            "\n",
            "       [[-0.71919304, -0.35905002,  0.96404055],\n",
            "        [-0.88865243,  0.46421602,  0.74266139],\n",
            "        [ 0.74926668,  1.28307069,  0.42748649],\n",
            "        [-0.75004522, -1.7396375 , -1.86188578],\n",
            "        [ 0.73691447,  0.55018542, -0.10454013],\n",
            "        [-0.7124178 , -0.29259302,  0.31569156]]])\n",
            "np.matmul(x, y) = array([[[ 2.16597185, -3.22462923, -0.12038855],\n",
            "        [ 0.01137958,  0.69836625,  0.27260237],\n",
            "        [ 0.83332558, -1.80151504, -0.85247487]],\n",
            "\n",
            "       [[-1.28492361,  0.41090454,  0.12886747],\n",
            "        [ 0.84090678,  3.71771972,  3.44424693],\n",
            "        [-0.74730815,  3.67628554,  3.28725497]]])\n",
            "x@y = array([[[ 2.16597185, -3.22462923, -0.12038855],\n",
            "        [ 0.01137958,  0.69836625,  0.27260237],\n",
            "        [ 0.83332558, -1.80151504, -0.85247487]],\n",
            "\n",
            "       [[-1.28492361,  0.41090454,  0.12886747],\n",
            "        [ 0.84090678,  3.71771972,  3.44424693],\n",
            "        [-0.74730815,  3.67628554,  3.28725497]]])\n",
            "np.matmul(x, y).shape = (2, 3, 3)\n",
            "(x@y).shape = (2, 3, 3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "lQQc-KGurNqn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This doesn't look right (the expected shape is (B, T, N, H) but looks like (B, T, H) or (N, T, H). Why?\n",
        "\n",
        "For N-D arrays with N > 2,  `np.matmul` and `@` assume it to be a stack of 2-D matrices with dims equal to the last two dims.\n",
        "\n",
        "So, an array of shape (X, Y, Z) is considered a stack of X matrices of shape (Y, Z) and can be matmul-ed with any array of shape (X, Z, K); it'll perform X matrix multiplications of shapes (Y, Z) and (Z, K) and stack them to produce an output of shape (X, Z, K). The stacked dims must be the same and the last two dims must be compatible for matrix multiplication.\n",
        "\n",
        "In our example above, the matmul works because B == N; and produces a stack of B (or N) (T, H) matrices. But this is not what we want.\n",
        "\n",
        "Let's see what `np.dot` does:"
      ],
      "metadata": {
        "id": "4LN0Ns1ZrLl_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "B, T, F, N, H = 2, 3, 6, 2, 3\n",
        "x = np.random.randn(B, T, F)\n",
        "y = np.random.randn(N, F, H)\n",
        "print(f'{x = }')\n",
        "print(f'{y = }')\n",
        "\n",
        "print(f'{np.dot(x, y) = }')\n",
        "print(f'{np.dot(x, y).shape = }')\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QYVT4cg6ru3j",
        "outputId": "b2d2959b-5103-48df-fafc-e036ced94458"
      },
      "execution_count": 136,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x = array([[[-0.30724707, -0.64317837,  0.12407669, -1.47495083,\n",
            "         -0.63610308, -0.07598378],\n",
            "        [-0.39526052,  1.02930852, -0.09945418,  0.97169454,\n",
            "         -0.02901918,  1.48830061],\n",
            "        [-0.73599041, -0.95060416,  0.61814902,  0.0859324 ,\n",
            "         -0.23626303, -0.80683889]],\n",
            "\n",
            "       [[-0.23720876, -0.72729933,  1.7624703 ,  2.23306648,\n",
            "         -0.90291396,  0.06278956],\n",
            "        [ 1.28283165, -1.69511569,  1.99807424, -1.4368085 ,\n",
            "         -1.45391621,  2.91306666],\n",
            "        [-0.07652882,  0.72713135, -0.16188935,  1.39061656,\n",
            "          0.63965567, -1.4441359 ]]])\n",
            "y = array([[[-0.86167241, -1.17096547, -1.53659739],\n",
            "        [ 0.84306895,  0.45486059,  0.90276171],\n",
            "        [ 0.1698639 ,  0.36172966, -0.70990293],\n",
            "        [-0.49363103,  1.59567673, -0.57092044],\n",
            "        [-1.75189101,  2.01801101, -0.03664438],\n",
            "        [ 0.71743567, -0.04832505, -2.06645815]],\n",
            "\n",
            "       [[ 0.20230371,  0.83633697,  0.39320103],\n",
            "        [-2.3415595 , -1.13446228, -0.37616085],\n",
            "        [-1.48476207, -1.55623921, -0.71452975],\n",
            "        [-1.68184483,  0.66106165,  0.47254469],\n",
            "        [ 0.22061245,  0.45201933, -0.98408195],\n",
            "        [ 0.77266047, -1.48177471, -0.45379171]]])\n",
            "np.dot(x, y) = array([[[[ 1.53153005e+00, -3.52143437e+00,  8.25802313e-01],\n",
            "         [ 3.54125535e+00, -8.70366971e-01, -4.04964891e-03]],\n",
            "\n",
            "        [[ 1.83040927e+00,  2.31507990e+00, -2.02202839e+00],\n",
            "         [-2.83317441e+00, -2.91960161e+00, -6.59191566e-01]],\n",
            "\n",
            "        [[-2.69607847e-01,  3.52359281e-01,  1.46082204e+00],\n",
            "         [ 3.39138357e-01,  6.46465472e-01,  2.65747911e-01]]],\n",
            "\n",
            "\n",
            "       [[[ 4.15156262e-01,  2.32260885e+00, -2.91483503e+00],\n",
            "         [-4.86817297e+00, -1.14109812e+00,  8.36245000e-01]],\n",
            "\n",
            "        [[ 3.15121455e+00, -6.91790508e+00, -1.00660692e+01],\n",
            "         [ 5.60861941e+00, -6.03708283e+00, -8.55745642e-01]],\n",
            "\n",
            "        [[-2.19166762e+00,  3.94139044e+00,  3.05582126e+00],\n",
            "         [-4.79124821e+00,  2.71133563e+00,  4.95056971e-01]]]])\n",
            "np.dot(x, y).shape = (2, 3, 2, 3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This does the correct thing, but let's dive deeper into how `np.dot` operates. Quoting `np.dot(a, b)` doc: *If a is an N-D array and b is an M-D array (where M>=2), it is a sum product over the last axis of a and the second-to-last axis of b:*.\n",
        "\n",
        "This means that the dims have to be correctly lined up, as is the case in our example: (B, T **F**) and (N, **F**, H).\n",
        "\n",
        "But if, for instance, the query projection matrix shape was (F, N, H) instead (not uncommon), then `np.dot` would fail (try flipping the shape of y above and check it out), or worse, fail silently if F == N (this is never going to be true for this example, because model dim is going to be much larger (e.g. 8k) than number of heads (e.g. 64), but this could happen in other scenarios).\n",
        "\n",
        "Hence, it's often safest to use `np.einsum`, which requires you to carefully define the shapes of the operands and output and provide the contracting, non-contracting and batch dims, but also doesn't require you (or readers of your code) to remember the nuances of `np.matmul` and `np.dot` behavior. It also enhances the  readability and debuggability of your code. You're welcome `np.einsum` for this endorsement!\n",
        "\n",
        "Let's see this in action!"
      ],
      "metadata": {
        "id": "pTzDiC-NvztZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "B, T, F, N, H = 2, 3, 6, 2, 3\n",
        "x = np.random.randn(B, T, F)\n",
        "y = np.random.randn(F, N, H)\n",
        "print(f'{x = }')\n",
        "print(f'{y = }')\n",
        "print(f'{np.einsum(\"btf,fnh->btnh\", x, y) = }')\n",
        "\n",
        "print(f'{x.shape = }')\n",
        "print(f'{y.shape = }')\n",
        "print(f'{np.einsum(\"btf,fnh->btnh\", x, y).shape = }')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qU9UN-zKvN6E",
        "outputId": "69e2b2e3-21d7-4e9f-bfad-ea69f8ed1fb2"
      },
      "execution_count": 137,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x = array([[[-0.29043058, -0.17204584, -0.86158297, -0.49222998,\n",
            "          1.92379191,  2.60295596],\n",
            "        [-2.2531825 , -0.789602  , -1.65644952, -0.88949568,\n",
            "         -1.39152864,  1.01644512],\n",
            "        [-1.26310646,  0.51355587, -1.57350882,  1.99960548,\n",
            "          0.92131171, -0.17867745]],\n",
            "\n",
            "       [[ 0.79842856, -1.16912592,  0.3913615 , -1.91401799,\n",
            "         -0.98391488, -1.24111122],\n",
            "        [ 0.72336014, -1.61726445,  0.9428253 , -0.72242463,\n",
            "         -0.04507699, -1.00701907],\n",
            "        [ 0.5286586 ,  1.46150133,  2.42470648,  0.76790258,\n",
            "          1.22730962,  0.24688453]]])\n",
            "y = array([[[-0.15635573, -0.06675993,  0.03178532],\n",
            "        [-0.81575918, -0.63310438,  0.1893891 ]],\n",
            "\n",
            "       [[ 0.52726784, -0.35260767, -0.93815809],\n",
            "        [-0.48407004, -0.12046975,  0.23237503]],\n",
            "\n",
            "       [[-0.89419184, -0.62368114,  0.02288441],\n",
            "        [ 0.78529318,  1.04499723, -1.28382101]],\n",
            "\n",
            "       [[-0.40142172, -1.54149619,  0.45608341],\n",
            "        [ 1.53528759,  0.18799954, -0.03939823]],\n",
            "\n",
            "       [[-0.63668288, -0.16827   ,  0.06671545],\n",
            "        [ 0.89478595,  0.99825146, -0.74222638]],\n",
            "\n",
            "       [[ 1.1079551 ,  0.14987095, -0.79776838],\n",
            "        [ 0.90499182,  0.28996211,  0.51125618]]])\n",
            "np.einsum(\"btf,fnh->btnh\", x, y) = array([[[[ 2.58182147,  1.44256851, -2.04024928],\n",
            "         [ 2.96492966,  1.88689503,  0.93341596]],\n",
            "\n",
            "        [[ 3.78635079,  3.21958069, -0.67816196],\n",
            "         [-0.77139896, -1.470952  ,  3.10391037]],\n",
            "\n",
            "        [[ 0.28805912, -2.37958298,  0.55804217],\n",
            "         [ 3.27877255,  0.33731391,  1.04626993]]],\n",
            "\n",
            "\n",
            "       [[[-1.071559  ,  3.04486322,  1.18268444],\n",
            "         [-4.72020984, -1.65757681, -0.4517294 ]],\n",
            "\n",
            "        [[-2.60593496,  0.90422303,  2.03269272],\n",
            "         [-1.12763198,  0.24930776, -1.90215941]],\n",
            "\n",
            "        [[-2.29633171, -3.41611097, -1.06367627],\n",
            "         [ 3.26593751,  3.46416487, -3.48812486]]]])\n",
            "x.shape = (2, 3, 6)\n",
            "y.shape = (6, 2, 3)\n",
            "np.einsum(\"btf,fnh->btnh\", x, y).shape = (2, 3, 2, 3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Works like magic, even though the dims aren't lined up perfectly!"
      ],
      "metadata": {
        "id": "aXgYNdBk3bql"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Einsum\n",
        "We just saw `np.einsum` in action. It uses Einstein summation convention to denote operations. Here are a few examples of simple operations using einsum:"
      ],
      "metadata": {
        "id": "15tQMu4f2N0a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = np.random.randn(2, 3)\n",
        "print(f'{x = }')\n",
        "\n",
        "print(\"# sum over an axis\")\n",
        "print(f'{np.sum(x, axis=1) = }')\n",
        "print(f'{np.einsum(\"ab->a\", x) = }')\n",
        "\n",
        "print()\n",
        "\n",
        "print(\"# global sum\")\n",
        "print(f'{np.sum(x) = }')\n",
        "print(f'{np.einsum(\"ab->\", x) = }')\n",
        "\n",
        "print()\n",
        "\n",
        "print(\"# transpose\")\n",
        "print(f'{np.reshape(x, (3, 2)) = }')\n",
        "print(f'{np.einsum(\"ab->ba\", x) = }')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cmBNkBU8ztEH",
        "outputId": "630e0991-30e3-4c39-8e3e-f5bda638894f"
      },
      "execution_count": 138,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x = array([[ 0.92021082, -2.43569954,  0.53765144],\n",
            "       [-0.32576371, -1.92305665,  0.81970867]])\n",
            "# sum over an axis\n",
            "np.sum(x, axis=1) = array([-0.97783729, -1.42911169])\n",
            "np.einsum(\"ab->a\", x) = array([-0.97783729, -1.42911169])\n",
            "\n",
            "# global sum\n",
            "np.sum(x) = np.float64(-2.406948982809228)\n",
            "np.einsum(\"ab->\", x) = np.float64(-2.406948982809228)\n",
            "\n",
            "# transpose\n",
            "np.reshape(x, (3, 2)) = array([[ 0.92021082, -2.43569954],\n",
            "       [ 0.53765144, -0.32576371],\n",
            "       [-1.92305665,  0.81970867]])\n",
            "np.einsum(\"ab->ba\", x) = array([[ 0.92021082, -0.32576371],\n",
            "       [-2.43569954, -1.92305665],\n",
            "       [ 0.53765144,  0.81970867]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The best use of einsum is for multiplying nulti-dimensional arrays by describing the input and output shapes, and denoting the batch, contracting and non-contracting dimensions. Batch dimensions appear in both inputs and the output, non-contracting dimensions appear in one of the inputs and the output, contracting dimensions appear in both inputs but not in the output."
      ],
      "metadata": {
        "id": "rDPiHDUg0_9Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "That's all for now! Thanks for tuning in!"
      ],
      "metadata": {
        "id": "upW1K5U71UGO"
      }
    }
  ]
}